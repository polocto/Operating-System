# Introduction to Threads

In precedent chapters, we have seen:
- How to take a **single physical CPU** and turn it into **multiple virtual CPUs**.
- How to create a **virtual memory** for each **process**

In this chapter will introduce:
- **Threads** (a new abstraction for a single running process)
- **Multi-threads** ("one program", multiple process with *shared memory*)

The context switch between threads is quite similar to the context switch between processes, as the register state of T1 must be saved and the register state of T2 restored before running T2.

We saved state to a **process control block** (**PCB**), we'll need one or more **thread control block** (**TCB**) to store the state of each thread of a process.

- **single-threaded process** = ***classic process***

With **multi-threaded process** instead of a single stack in the address space, there will be one per
thread.

Thus, any **stack-allocated *variables***, ***parameters***, **return** ***values***, and other ***things*** that we put on the stack will be placed in what is sometimes called **thread-local storage**.

>You might also notice how this ruins our beautiful address space lay-out. Before, the stack and heap could grow independently and trouble only arose when you ran out of room in the address space. Here, we no longer have such a nice situation. Fortunately, this is usually OK, as stacks do not generally have to be very large (the exception being in programs that make heavy use of recursion).

## Sommaire
- [Why Use Threads](#why-use-threads)
- [Thread Creation](#thread-creation)
- [Why It Gets Worse: Shared Data](#why-it-gets-worse-shared-data)
- [The Heart Of The Problem: Uncontrolled Scheduling](#the-heart-of-the-problem-uncontrolled-scheduling)
- [The Wish For Atomicity](#the-wish-for-atomicity)
- [Waiting For Another](#waiting-for-another)
- [Summary](#summary)

## Why Use Threads

- **Parallelism**
- **Avoid waiting** due to **I/O**

## Thread Creation


```c
#include <stdio.h>
#include <assert.h>
#include <pthread.h>
#include "common.h"
#include "common_threads.h"

void *mythread(void *arg) {
    printf("%s\n", (char *) arg);
    return NULL;
}
int main(int argc, char *argv[]) {
    pthread_t p1, p2;
    int rc;
    printf("main: begin\n");
    Pthread_create(&p1, NULL, mythread, "A");
    Pthread_create(&p2, NULL, mythread, "B");
    // join waits for the threads to finish
    Pthread_join(p1, NULL);
    Pthread_join(p2, NULL);
    printf("main: end\n");
    return 0;
}
```

Alternately **threads** (***main & T1 & T2***) may be put in a **ready** state.
### Thread Trace (1)
main            | Thread 1  | Thread 2
:-              | :-        | :-
starts running
prints "main: begin"
creates Thread 1
creates Thread 2
waits for T1
 |              |runs
 |              |prints "A"
 |              |returns
 waits for T2
 |              |           |runs
 |              |           |prints "B"
 |              |           |returns
 prints "main: end"

### Thread Trace (2)
main            | Thread 1  | Thread 2
:-              | :-        | :-
starts running
prints "main: begin"
creates Thread 1
 |              |runs
 |              |prints "A"
 |              |returns
creates Thread 2
 |              |           |runs
 |              |           |prints "B"
 |              |           |returns
waits for T1
*returns immediately; T1 is done*
waits for T2 
*returns immediately; T2 is done*
 prints "main: end"

### Thread Trace (3)
> It's already hard to determine which process will run... Unfotunately it is even harder with **concurrency**.

main            | Thread 1  | Thread 2
:-              | :-        | :-
starts running
prints "main: begin"
creates Thread 1
creates Thread 2
 |              |           |runs
 |              |           |prints "B"
 |              |           |returns
waits for T1
 |              |runs
 |              |prints "A"
 |              |returns
waits for T2 
*returns immediately; T2 is done*
 prints "main: end"


 ## Why It Gets Worse: Shared Data

> What happens if two different threads want to change a shared data ?

```c
#include <stdio.h>
#include <pthread.h>
#include "common.h"
#include "common_threads.h"
static volatile int counter = 0;
// mythread()
//
// Simply adds 1 to counter repeatedly, in a loop
// No, this is not how you would add 10,000,000 to
// a counter, but it shows the problem nicely.
//
void *mythread(void *arg) {
    printf("%s: begin\n", (char *) arg);
    int i;
    for (i = 0; i < 1e7; i++) {
        counter = counter + 1;
    }
    printf("%s: done\n", (char *) arg);
    return NULL;
}

// main()
//
// Just launches two threads (pthread_create)
// and then waits for them (pthread_join)
//
int main(int argc, char *argv[]) {
    pthread_t p1, p2;
    printf("main: begin (counter = %d)\n", counter);
    Pthread_create(&p1, NULL, mythread, "A");
    Pthread_create(&p2, NULL, mythread, "B");
    // join waits for the threads to finish
    Pthread_join(p1, NULL);
    Pthread_join(p2, NULL);
    printf("main: done with both (counter = %d)\n", counter);
    return 0;
}
```

Unfortunately, when we run this code, even on a single processor, we don’t necessarily get the desired result. Not only is **each run wrong**, but also yields a different result! A big question remains: 
>Why does this happen?

    Tip
>You should always learn new tools that help you write, debug, and understand computer systems. Here, we use a neat tool called a disassembler. When you run a disassembler on an executable, it shows you what
assembly instructions make up the program. For example, if we wish to understand the low-level code to update a counter (as in our example), we run objdump (Linux) to see the assembly code:
>
>`prompt> objdump -d main`
>
>Doing so produces a long listing of all the instructions in the program, neatly labeled (particularly if you compiled with the -g flag), which includes symbol information in the program. The objdump program is just one of many tools you should learn how to use; a debugger like gdb, memory profilers like valgrind or purify, and of course the compiler itself are others that you should spend time to learn more about; the better you are at using your tools, the better systems you’ll be able to build.

## The Heart Of The Problem: Uncontrolled Scheduling

If there is **multiple threads** that need to access a same part of code, there's a **risk** that shared memory is **not changed in the "good" ordre**. Making the result **incorrect**. 

*Example the code to increment counter has been run twice, but counter, which started at 50, is **now only equal to 51**. A “correct” version of this program **should have** resulted in the variable counter equal to **52**.*

>The result **depends** on the **timing execution**

What we have demonstrated here is called a **[race condition](#glossary)** (or, more specifically, a **data race**): the results **depend on the timing execution** of the code. The result is **[indeterminate](#glossary)**. **Multiple threads** executing this code can result in a **race condition**, this code is called **[critical section](#glossary)**.

What we really want for this code is what we call **[mutual exclusion](#glossary)**. This property guarantees that if one thread is executing within the critical section, the **others will be prevented from doing so**. 

## The Wish For Atomicity

>Atomic operations are one of the most powerful underlying techniques in building computer systems, from the computer architecture, to concurrent code (what we are studying here), to file systems (which we’ll study soon enough), database management systems, and even distributed systems [L+93]. The idea behind making a series of actions atomic is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a transaction, an idea developed in great detail in the world of databases and transaction processing [GR92].
>
>In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution, but the idea of atomicity is much bigger than that, as we will
see. For example, file systems use techniques such as journaling or copy on-write in order to atomically transition their on-disk state, critical for operating correctly in the face of system failures. If that doesn’t make sense, don’t worry — it will, in some future chapter.


A possibility to avoid **data race** is to have a **super instruction** that does multiple instruction in one.

`memory-add 0x8049a1c, $0x1`

Assume this instruction adds a value to a memory location, and the hardware guarantees that it executes **atomically**; when the instruction executed, it would perform the update as desired. It **could not be interrupted mid-instruction**, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has **not run at all**, or it has **run to completion**; *there is no in-between state*.

> **Atomically** = as a unit *all or none*

We don't nececessarly want an atomic instruction. We will instead ask the **hardware** for a few useful instructions upon which we can build a general set of what we call **synchronization primitive**.

By using the support of the **hardware** we will be able to build **multi-threads** code that **accesses critical sections** in a **synchronized and controlled manner**.

## Waiting For Another

There is more problems than just **data race**, **threads** may **wait one another** before it continues. For example, when a **process performs** a disk **I/O** and is put to **sleep**; when the **I/O completes**, the **process** needs to be **roused** from its [slumber](https://www.deepl.com/fr/translator#en/fr/slumber%0A) so it can continue.


## Summary
Before wrapping up, one question that you might have is: 
>Why are we studying this in OS class? 

**“History”** is the one-word answer; the **OS** was the **first concurrent program**, and many techniques were created for use with in the OS. Later, with **multi-threaded** processes, application programmers also **had to consider** such things. 

For example, imagine the case where there are **two processes running**. Assume they both call ***write()*** to write to the file, and both wish to append the data to the file. 

To do so, both **must allocate a new block**, record in the [inode](https://en.wikipedia.org/wiki/Inode) of the file where this block lives, and change the size of the file to reflect the new larger size (among other things; we’ll learn more about files in the third part of the book). 

Because an **interrupt may occurat** any time, the **code that updates** these shared structures are **critical sections**; 
thus, OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. An untimely interrupt causes all of the problems described above. 

Not surprisingly, [page tables](https://en.wikipedia.org/wiki/Page_table), **process lists**, **file system structures**, and virtually every [kernel datastructure](https://www.tutorialspoint.com/Kernel-Data-Structures) has to be carefully accessed, with the proper synchronization primitives, to work correctly.

-----------------------------

|Thus, in the coming chapters, we’ll be not only studying how to buildsupport for synchronization primitives to support atomicity but also formechanisms to support this type of sleeping/waking interaction that iscommon in multi-threaded programs.|
|:-:|


## Glossary

- A **critical sections** a piece of code that accesses a *shared* resource,usually a variable or data structure.
- A **race condition** (or **data race** [NM92]) arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome.
- An **indeterminate** program consists of one or more race conditions;the output of the program varies from run to run, depending onwhich threads ran when. The outcome is thus notdeterministic,something we usually expect from computer systems.
- To avoid these problems, threads should use some kind of **mutual exclusion** primitives; doing so guarantees that only a single threadever enters a critical section, thus avoiding races, and resulting indeterministic program outputs.